{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff08075-3153-4176-a4fc-b6ad4be2e3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdc683623a1443b96bf1c209add99a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'statements'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Choose metric(s)\u001b[39;00m\n\u001b[32m     23\u001b[39m metric = AnswerRelevancyMetric(model=llm, threshold=\u001b[32m0.7\u001b[39m, include_reason=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m score = \u001b[43mmetric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mScore:\u001b[39m\u001b[33m\"\u001b[39m, score)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mReason:\u001b[39m\u001b[33m\"\u001b[39m, metric.reason \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(metric, \u001b[33m\"\u001b[39m\u001b[33mreason\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Projects\\Gen AI Projects\\summary-classification-poc\\.venv\\Lib\\site-packages\\deepeval\\metrics\\answer_relevancy\\answer_relevancy.py:63\u001b[39m, in \u001b[36mAnswerRelevancyMetric.measure\u001b[39m\u001b[34m(self, test_case, _show_indicator, _in_component)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_mode:\n\u001b[32m     62\u001b[39m     loop = get_or_create_event_loop()\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ma_measure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_show_indicator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_in_component\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_in_component\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mself\u001b[39m.statements: List[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28mself\u001b[39m._generate_statements(\n\u001b[32m     72\u001b[39m         test_case.actual_output\n\u001b[32m     73\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Projects\\Gen AI Projects\\summary-classification-poc\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\asyncio\\futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Projects\\Gen AI Projects\\summary-classification-poc\\.venv\\Lib\\site-packages\\deepeval\\metrics\\answer_relevancy\\answer_relevancy.py:107\u001b[39m, in \u001b[36mAnswerRelevancyMetric.a_measure\u001b[39m\u001b[34m(self, test_case, _show_indicator, _in_component)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluation_cost = \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.using_native_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m metric_progress_indicator(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    103\u001b[39m     async_mode=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    104\u001b[39m     _show_indicator=_show_indicator,\n\u001b[32m    105\u001b[39m     _in_component=_in_component,\n\u001b[32m    106\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28mself\u001b[39m.statements: List[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._a_generate_statements(\n\u001b[32m    108\u001b[39m         test_case.actual_output\n\u001b[32m    109\u001b[39m     )\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m.verdicts: List[AnswerRelevancyVerdict] = (\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._a_generate_verdicts(test_case.input)\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m.score = \u001b[38;5;28mself\u001b[39m._calculate_score()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Projects\\Gen AI Projects\\summary-classification-poc\\.venv\\Lib\\site-packages\\deepeval\\metrics\\answer_relevancy\\answer_relevancy.py:257\u001b[39m, in \u001b[36mAnswerRelevancyMetric._a_generate_statements\u001b[39m\u001b[34m(self, actual_output)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    254\u001b[39m     res: Statements = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.a_generate(\n\u001b[32m    255\u001b[39m         prompt, schema=Statements\n\u001b[32m    256\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatements\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    259\u001b[39m     res = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.a_generate(prompt)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'statements'"
     ]
    }
   ],
   "source": [
    "# evaluate_openrouter.py\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.utils.openrouter_deepeval_llm import OpenRouterDeepevalLLM\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric, ContextualPrecisionMetric\n",
    "\n",
    "# Initialize custom LLM\n",
    "llm = OpenRouterDeepevalLLM()\n",
    "\n",
    "# Test case\n",
    "test_case = LLMTestCase(\n",
    "    input=\"Explain how photosynthesis works.\",\n",
    "    expected_output=\"Plants use sunlight to convert carbon dioxide and water into glucose and oxygen.\",\n",
    "    actual_output=None # you might generate that\n",
    ")\n",
    "\n",
    "# Let's generate actual_output\n",
    "test_case.actual_output = llm.generate(test_case.input)\n",
    "\n",
    "# Choose metric(s)\n",
    "metric = AnswerRelevancyMetric(model=llm, threshold=0.7, include_reason=True)\n",
    "\n",
    "score = metric.measure(test_case)\n",
    "print(\"Score:\", score)\n",
    "print(\"Reason:\", metric.reason if hasattr(metric, \"reason\") else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3358b7c1-f083-4406-98fb-65bd8eb27a7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unsupported type for model: <class 'src.utils.ilm_client.OpenRouterILMClient'>. Expected None, str, DeepEvalBaseLLM, GPTModel, AzureOpenAIModel, LiteLLMModel, OllamaModel, LocalModel.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# Run assertion\u001b[39;00m\n\u001b[32m     42\u001b[39m     assert_test(test_case, [correctness_metric])\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mtest_sample_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtest_sample_1\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_sample_1\u001b[39m():\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Define metric using our client\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     correctness_metric = \u001b[43mGEval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcorrectness\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDoes the response correctly answer the question based on the provided context?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluation_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mLLMTestCaseParams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mACTUAL_OUTPUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLLMTestCaseParams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEXPECTED_OUTPUT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43milm_client\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ðŸ‘ˆ use our OpenRouterILMClient\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# Input data\u001b[39;00m\n\u001b[32m     25\u001b[39m     input_data = {\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhat is 2 + 2?\u001b[39m\u001b[33m\"\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Projects\\Gen AI Projects\\summary-classification-poc\\.venv\\Lib\\site-packages\\deepeval\\metrics\\g_eval\\g_eval.py:58\u001b[39m, in \u001b[36mGEval.__init__\u001b[39m\u001b[34m(self, name, evaluation_params, criteria, evaluation_steps, rubric, model, threshold, top_logprobs, async_mode, strict_mode, verbose_mode, evaluation_template, _include_g_eval_suffix)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m.score_range = get_score_range(\u001b[38;5;28mself\u001b[39m.rubric)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mself\u001b[39m.score_range_span = \u001b[38;5;28mself\u001b[39m.score_range[\u001b[32m1\u001b[39m] - \u001b[38;5;28mself\u001b[39m.score_range[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.using_native_model = \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluation_model = \u001b[38;5;28mself\u001b[39m.model.get_model_name()\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluation_steps = evaluation_steps\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Projects\\Gen AI Projects\\summary-classification-poc\\.venv\\Lib\\site-packages\\deepeval\\metrics\\utils.py:472\u001b[39m, in \u001b[36minitialize_model\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    469\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GPTModel(model=model), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# Otherwise (the model is a wrong type), we raise an error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    473\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported type for model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Expected None, str, DeepEvalBaseLLM, GPTModel, AzureOpenAIModel, LiteLLMModel, OllamaModel, LocalModel.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    474\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: Unsupported type for model: <class 'src.utils.ilm_client.OpenRouterILMClient'>. Expected None, str, DeepEvalBaseLLM, GPTModel, AzureOpenAIModel, LiteLLMModel, OllamaModel, LocalModel."
     ]
    }
   ],
   "source": [
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "from dotenv import load_dotenv\n",
    "from src.utils.ilm_client import OpenRouterILMClient  # ðŸ‘ˆ use your client\n",
    "import os\n",
    "\n",
    "# Load .env\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenRouter LLM client\n",
    "ilm_client = OpenRouterILMClient()  # automatically loads API key & model from .env\n",
    "\n",
    "def test_sample_1():\n",
    "    # Define metric using our client\n",
    "    correctness_metric = GEval(\n",
    "        name=\"correctness\",\n",
    "        criteria=\"Does the response correctly answer the question based on the provided context?\",\n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "        threshold=0.5,\n",
    "        model=ilm_client  # ðŸ‘ˆ use our OpenRouterILMClient\n",
    "    )\n",
    "\n",
    "    # Input data\n",
    "    input_data = {\"question\": \"What is 2 + 2?\"}\n",
    "    \n",
    "    # Get actual output from OpenRouter\n",
    "    actual_output = ilm_client.generate(input_data[\"question\"])\n",
    "    print(f\"Actual Output: {actual_output}\")\n",
    "    \n",
    "    # Build LLMTestCase\n",
    "    test_case = LLMTestCase(\n",
    "        name=\"Sample Test Case 1\",\n",
    "        description=\"Test case to evaluate the correctness of a simple addition operation.\",\n",
    "        input=input_data[\"question\"],                # the prompt\n",
    "        expected_output=\"4\",                          # expected answer\n",
    "        actual_output=actual_output.strip(),         # remove leading/trailing whitespace\n",
    "        metrics=[correctness_metric]\n",
    "    )\n",
    "\n",
    "    # Run assertion\n",
    "    assert_test(test_case, [correctness_metric])\n",
    "\n",
    "    \n",
    "test_sample_1()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5b77e76-4aac-4e7f-8939-86c27f58af4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dc864b9cc049a7997840326975117a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.models.llms import LiteLLMModel\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load env\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Use LiteLLMModel for OpenRouter\n",
    "llm = LiteLLMModel(model=\"openrouter/nvidia/nemotron-nano-9b-v2:free\")\n",
    "\n",
    "def test_sample_1():\n",
    "    correctness_metric = GEval(\n",
    "        name=\"correctness\",\n",
    "        criteria=\"Does the response correctly answer the question?\",\n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "        threshold=0.5,\n",
    "        model=llm\n",
    "    )\n",
    "\n",
    "    question = \"What is 2 + 2?\"\n",
    "    actual_output = llm.generate(question)  # uses OpenRouter under the hood\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        name=\"Sample Test Case 1\",\n",
    "        description=\"Simple addition test.\",\n",
    "        input=question,\n",
    "        expected_output=\"4\",\n",
    "        actual_output=actual_output[0].strip(),\n",
    "        metrics=[correctness_metric]\n",
    "    )\n",
    "\n",
    "    assert_test(test_case, [correctness_metric])\n",
    "\n",
    "test_sample_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f14ac6-1abc-465b-ad83-2f949c4735ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
