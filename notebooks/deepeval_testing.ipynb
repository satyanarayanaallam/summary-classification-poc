{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepeval Integration Testing Notebook\n",
    "\n",
    "This notebook provides comprehensive testing of the Deepeval framework integration for summary classification.\n",
    "\n",
    "## Features to Test:\n",
    "1. **Single Prediction with Live Evaluation**\n",
    "2. **Batch Evaluation with Multiple Metrics**\n",
    "3. **Custom Test Cases**\n",
    "4. **Edge Case Testing**\n",
    "5. **Dataset Evaluation**\n",
    "6. **Performance Analysis**\n",
    "\n",
    "## Prerequisites:\n",
    "- Install dependencies: `pip install -r requirements.txt`\n",
    "- Set Gemini API key in `.env` file for Deepeval\n",
    "- Run this notebook from project root directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Working directory: D:\\Projects\\Gen AI Projects\\summary-classification-poc\\notebooks\n",
      "‚úì Added to path: D:\\Projects\\Gen AI Projects\\summary-classification-poc\\src\n",
      "‚úì Python path includes: ['D:\\\\Projects\\\\Gen AI Projects\\\\summary-classification-poc\\\\src']\n",
      "Warning: Deepeval not available. Install with: pip install deepeval\n",
      "‚úì CrewAIAgent imported successfully\n",
      "‚úì Deepeval test cases imported successfully\n",
      "‚úì EvaluationService imported successfully\n",
      "\n",
      "üéâ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path - IMPORTANT: Run this notebook from project root directory\n",
    "src_path = str(Path('../src').resolve())\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "print(f\"‚úì Working directory: {Path.cwd()}\")\n",
    "print(f\"‚úì Added to path: {src_path}\")\n",
    "print(f\"‚úì Python path includes: {[p for p in sys.path if 'src' in p]}\")\n",
    "\n",
    "# Import our modules\n",
    "try:\n",
    "    from agents.crew_ai_agent import CrewAIAgent\n",
    "    print(\"‚úì CrewAIAgent imported successfully\")\n",
    "    \n",
    "    from evaluation.deepeval_test_cases import (\n",
    "        SummaryClassificationTestCases,\n",
    "        create_custom_test_cases,\n",
    "        create_edge_case_test_cases\n",
    "    )\n",
    "    print(\"‚úì Deepeval test cases imported successfully\")\n",
    "    \n",
    "    from services.evaluation_service import EvaluationService\n",
    "    print(\"‚úì EvaluationService imported successfully\")\n",
    "    \n",
    "    print(\"\\nüéâ All imports successful!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure you're running this notebook from the project root directory\")\n",
    "    print(\"2. Check that the src directory exists and contains all modules\")\n",
    "    print(\"3. Verify all __init__.py files are present\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Test Basic Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì CrewAIAgent import successful\n",
      "‚úì EvaluationService import successful\n",
      "‚úì SummaryClassificationTestCases import successful\n",
      "‚ö† Using fallback evaluation (install deepeval for advanced metrics)\n",
      "‚úì EvaluationService initialization successful\n",
      "Error loading test cases: 'context' must be None or a list of strings\n",
      "‚úì SummaryClassificationTestCases initialization successful\n",
      "\n",
      "üéâ All imports and initializations successful!\n",
      "Ready to proceed with Deepeval testing.\n"
     ]
    }
   ],
   "source": [
    "# Test basic imports to ensure everything is working\n",
    "try:\n",
    "    # Test individual imports\n",
    "    from agents.crew_ai_agent import CrewAIAgent\n",
    "    print(\"‚úì CrewAIAgent import successful\")\n",
    "    \n",
    "    from services.evaluation_service import EvaluationService\n",
    "    print(\"‚úì EvaluationService import successful\")\n",
    "    \n",
    "    from evaluation.deepeval_test_cases import SummaryClassificationTestCases\n",
    "    print(\"‚úì SummaryClassificationTestCases import successful\")\n",
    "    \n",
    "    # Test initialization\n",
    "    evaluator = EvaluationService()\n",
    "    print(\"‚úì EvaluationService initialization successful\")\n",
    "    \n",
    "    test_cases = SummaryClassificationTestCases()\n",
    "    print(\"‚úì SummaryClassificationTestCases initialization successful\")\n",
    "    \n",
    "    print(\"\\nüéâ All imports and initializations successful!\")\n",
    "    print(\"Ready to proceed with Deepeval testing.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure you're running this notebook from the project root directory\")\n",
    "    print(\"2. Check that all dependencies are installed: pip install -r requirements.txt\")\n",
    "    print(\"3. Verify the src directory structure is correct\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Initialize Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CrewAI Agent...\n",
      "Using Sentence Transformers model 'all-MiniLM-L6-v2' with dimension 384\n",
      "‚ö† Using fallback evaluation (install deepeval for advanced metrics)\n",
      "Indexed 300 triplets from 300 summaries\n",
      "‚úì Agent initialized successfully\n",
      "\n",
      "Initializing Evaluation Service...\n",
      "‚ö† Using fallback evaluation (install deepeval for advanced metrics)\n",
      "‚úì Evaluation service initialized\n",
      "\n",
      "Loading test cases...\n",
      "Error loading test cases: 'context' must be None or a list of strings\n",
      "‚úì Loaded 0 test cases from dataset\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CrewAI Agent\n",
    "print(\"Initializing CrewAI Agent...\")\n",
    "try:\n",
    "    agent = CrewAIAgent()\n",
    "    print(\"‚úì Agent initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize agent: {e}\")\n",
    "    print(\"This might be due to missing data file or dependencies\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Initialize Evaluation Service\n",
    "print(\"\\nInitializing Evaluation Service...\")\n",
    "try:\n",
    "    evaluator = EvaluationService()\n",
    "    print(\"‚úì Evaluation service initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize evaluator: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Initialize Test Cases\n",
    "print(\"\\nLoading test cases...\")\n",
    "try:\n",
    "    test_cases_manager = SummaryClassificationTestCases()\n",
    "    print(f\"‚úì Loaded {len(test_cases_manager.get_test_cases())} test cases from dataset\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load test cases: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Single Prediction with Live Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Summary: Payment of $1200 was made by ACME Corp on 2025-09-01 for invoice #INV-100.\n",
      "Expected Document Type: INVOICE\n",
      "\n",
      "============================================================\n",
      "üìä PREDICTION RESULTS:\n",
      "Predicted Document Type: INVOICE\n",
      "Document Code: INV005\n",
      "Triplets Extracted: 4\n",
      "\n",
      "üîç EXTRACTED TRIPLETS:\n",
      "  1. ('amount', 'has_amount', '<AMOUNT>')\n",
      "  2. ('date', 'date', '<DATE>')\n",
      "  3. ('id', 'identifier', 'invoice')\n",
      "  4. ('was', 'made', 'by acme corp on <DATE> for invoice #inv-100')\n",
      "\n",
      "üìà EVALUATION METRICS:\n",
      "Framework: fallback\n",
      "‚ö† Using fallback evaluation (Deepeval not available)\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Test single prediction with ground truth\n",
    "test_summary = \"Payment of $1200 was made by ACME Corp on 2025-09-01 for invoice #INV-100.\"\n",
    "expected_doc_type = \"INVOICE\"\n",
    "\n",
    "print(f\"Testing Summary: {test_summary}\")\n",
    "print(f\"Expected Document Type: {expected_doc_type}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Run prediction with evaluation\n",
    "    result = agent.run(test_summary, ground_truth_doc_type=expected_doc_type)\n",
    "\n",
    "    # Display results\n",
    "    print(\"üìä PREDICTION RESULTS:\")\n",
    "    print(f\"Predicted Document Type: {result['summary_type']}\")\n",
    "    print(f\"Document Code: {result['doc_code']}\")\n",
    "    print(f\"Triplets Extracted: {len(result['triplets'])}\")\n",
    "\n",
    "    print(\"\\nüîç EXTRACTED TRIPLETS:\")\n",
    "    for i, triplet in enumerate(result['triplets'], 1):\n",
    "        print(f\"  {i}. {triplet}\")\n",
    "\n",
    "    print(\"\\nüìà EVALUATION METRICS:\")\n",
    "    metrics = result['metrics']\n",
    "    print(f\"Framework: {metrics.get('framework', 'N/A')}\")\n",
    "\n",
    "    if metrics.get('framework') == 'deepeval':\n",
    "        print(f\"Accuracy: {metrics.get('accuracy', 0):.3f}\")\n",
    "        print(f\"Contextual Precision: {metrics.get('contextual_precision', 0):.3f}\")\n",
    "        print(f\"Contextual Recall: {metrics.get('contextual_recall', 0):.3f}\")\n",
    "        print(f\"Contextual Relevancy: {metrics.get('contextual_relevancy', 0):.3f}\")\n",
    "        print(f\"Faithfulness: {metrics.get('faithfulness', 0):.3f}\")\n",
    "    else:\n",
    "        print(\"‚ö† Using fallback evaluation (Deepeval not available)\")\n",
    "        print(f\"Accuracy: {metrics.get('accuracy', 'N/A')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during single prediction test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepeval Integration Testing Notebook\n",
    "\n",
    "This notebook provides comprehensive testing of the Deepeval framework integration for summary classification.\n",
    "\n",
    "## Features to Test:\n",
    "1. **Single Prediction with Live Evaluation**\n",
    "2. **Batch Evaluation with Multiple Metrics**\n",
    "3. **Custom Test Cases**\n",
    "4. **Edge Case Testing**\n",
    "5. **Dataset Evaluation**\n",
    "6. **Performance Analysis**\n",
    "\n",
    "## Prerequisites:\n",
    "- Install dependencies: `pip install -r requirements.txt`\n",
    "- Set Gemini API key in `.env` file for Deepeval\n",
    "- Run this notebook from project root directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(Path(\u001b[33m'\u001b[39m\u001b[33m../src\u001b[39m\u001b[33m'\u001b[39m)))\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Import our modules\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcrew_ai_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrewAIAgent\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeepeval_test_cases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     SummaryClassificationTestCases,\n\u001b[32m     16\u001b[39m     create_custom_test_cases,\n\u001b[32m     17\u001b[39m     create_edge_case_test_cases\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mservices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation_service\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationService\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Projects\\Gen AI Projects\\summary-classification-poc\\notebooks\\..\\src\\agents\\crew_ai_agent.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriplet_service\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TripletService\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvector_store\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FaissVectorStore\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrieval_service\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalService\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path('../src')))\n",
    "\n",
    "# Import our modules\n",
    "from agents.crew_ai_agent import CrewAIAgent\n",
    "from evaluation.deepeval_test_cases import (\n",
    "    SummaryClassificationTestCases,\n",
    "    create_custom_test_cases,\n",
    "    create_edge_case_test_cases\n",
    ")\n",
    "from services.evaluation_service import EvaluationService\n",
    "\n",
    "print(\"‚úì All imports successful\")\n",
    "print(f\"‚úì Python version: {sys.version}\")\n",
    "print(f\"‚úì Working directory: {Path.cwd()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Initialize Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CrewAI Agent\n",
    "print(\"Initializing CrewAI Agent...\")\n",
    "agent = CrewAIAgent()\n",
    "print(\"‚úì Agent initialized successfully\")\n",
    "\n",
    "# Initialize Evaluation Service\n",
    "print(\"\\nInitializing Evaluation Service...\")\n",
    "evaluator = EvaluationService()\n",
    "print(\"‚úì Evaluation service initialized\")\n",
    "\n",
    "# Initialize Test Cases\n",
    "print(\"\\nLoading test cases...\")\n",
    "test_cases_manager = SummaryClassificationTestCases()\n",
    "print(f\"‚úì Loaded {len(test_cases_manager.get_test_cases())} test cases from dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Single Prediction with Live Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single prediction with ground truth\n",
    "test_summary = \"Payment of $1200 was made by ACME Corp on 2025-09-01 for invoice #INV-100.\"\n",
    "expected_doc_type = \"INVOICE\"\n",
    "\n",
    "print(f\"Testing Summary: {test_summary}\")\n",
    "print(f\"Expected Document Type: {expected_doc_type}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Run prediction with evaluation\n",
    "result = agent.run(test_summary, ground_truth_doc_type=expected_doc_type)\n",
    "\n",
    "# Display results\n",
    "print(\"üìä PREDICTION RESULTS:\")\n",
    "print(f\"Predicted Document Type: {result['summary_type']}\")\n",
    "print(f\"Document Code: {result['doc_code']}\")\n",
    "print(f\"Triplets Extracted: {len(result['triplets'])}\")\n",
    "\n",
    "print(\"\\nüîç EXTRACTED TRIPLETS:\")\n",
    "for i, triplet in enumerate(result['triplets'], 1):\n",
    "    print(f\"  {i}. {triplet}\")\n",
    "\n",
    "print(\"\\nüìà EVALUATION METRICS:\")\n",
    "metrics = result['metrics']\n",
    "print(f\"Framework: {metrics.get('framework', 'N/A')}\")\n",
    "\n",
    "if metrics.get('framework') == 'deepeval':\n",
    "    print(f\"Accuracy: {metrics.get('accuracy', 0):.3f}\")\n",
    "    print(f\"Contextual Precision: {metrics.get('contextual_precision', 0):.3f}\")\n",
    "    print(f\"Contextual Recall: {metrics.get('contextual_recall', 0):.3f}\")\n",
    "    print(f\"Contextual Relevancy: {metrics.get('contextual_relevancy', 0):.3f}\")\n",
    "    print(f\"Faithfulness: {metrics.get('faithfulness', 0):.3f}\")\n",
    "else:\n",
    "    print(\"‚ö† Using fallback evaluation (Deepeval not available)\")\n",
    "    print(f\"Accuracy: {metrics.get('accuracy', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Custom Test Cases Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test custom test cases\n",
    "print(\"üß™ Testing Custom Test Cases\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "custom_cases = create_custom_test_cases()\n",
    "print(f\"Created {len(custom_cases)} custom test cases\")\n",
    "\n",
    "# Convert to test data format\n",
    "custom_test_data = []\n",
    "for tc in custom_cases:\n",
    "    custom_test_data.append({\n",
    "        \"summary\": tc.input,\n",
    "        \"doc_type\": tc.expected_output\n",
    "    })\n",
    "\n",
    "# Display test cases\n",
    "print(\"\\nüìã CUSTOM TEST CASES:\")\n",
    "for i, test_data in enumerate(custom_test_data, 1):\n",
    "    print(f\"\\n{i}. Summary: {test_data['summary'][:60]}...\")\n",
    "    print(f\"   Expected: {test_data['doc_type']}\")\n",
    "\n",
    "# Run batch evaluation\n",
    "print(\"\\nüîÑ Running Batch Evaluation...\")\n",
    "batch_results = agent.run_batch_evaluation(custom_test_data)\n",
    "\n",
    "# Display results\n",
    "evaluation = batch_results[\"batch_evaluation\"]\n",
    "print(f\"\\nüìä BATCH EVALUATION RESULTS:\")\n",
    "print(f\"Framework: {evaluation.get('framework', 'N/A')}\")\n",
    "print(f\"Number of Test Cases: {evaluation.get('n', 0)}\")\n",
    "print(f\"Accuracy: {evaluation.get('accuracy', 0):.3f}\")\n",
    "\n",
    "if evaluation.get('framework') == 'deepeval':\n",
    "    print(f\"Contextual Precision: {evaluation.get('contextual_precision', 0):.3f}\")\n",
    "    print(f\"Contextual Recall: {evaluation.get('contextual_recall', 0):.3f}\")\n",
    "    print(f\"Contextual Relevancy: {evaluation.get('contextual_relevancy', 0):.3f}\")\n",
    "    print(f\"Faithfulness: {evaluation.get('faithfulness', 0):.3f}\")\n",
    "    print(f\"Overall Score: {evaluation.get('overall_score', 0):.3f}\")\n",
    "\n",
    "# Display individual results\n",
    "print(\"\\nüìã INDIVIDUAL RESULTS:\")\n",
    "individual_results = batch_results[\"individual_results\"]\n",
    "for i, result in enumerate(individual_results, 1):\n",
    "    status = \"‚úÖ\" if result[\"correct\"] else \"‚ùå\"\n",
    "    print(f\"{i}. {status} Predicted: {result['predicted']} | Actual: {result['actual']}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = sum(1 for r in individual_results if r[\"correct\"])\n",
    "total_predictions = len(individual_results)\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "print(f\"\\nüéØ Custom Test Cases Accuracy: {accuracy:.3f} ({correct_predictions}/{total_predictions})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Edge Cases Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test edge cases\n",
    "print(\"üîç Testing Edge Cases\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "edge_cases = create_edge_case_test_cases()\n",
    "print(f\"Created {len(edge_cases)} edge case test scenarios\")\n",
    "\n",
    "# Convert to test data format\n",
    "edge_test_data = []\n",
    "for tc in edge_cases:\n",
    "    edge_test_data.append({\n",
    "        \"summary\": tc.input,\n",
    "        \"doc_type\": tc.expected_output\n",
    "    })\n",
    "\n",
    "# Display edge cases\n",
    "print(\"\\nüìã EDGE CASES:\")\n",
    "for i, test_data in enumerate(edge_test_data, 1):\n",
    "    print(f\"\\n{i}. Summary: {test_data['summary'][:80]}...\")\n",
    "    print(f\"   Expected: {test_data['doc_type']}\")\n",
    "\n",
    "# Run evaluation on edge cases\n",
    "print(\"\\nüîÑ Running Edge Case Evaluation...\")\n",
    "edge_results = agent.run_batch_evaluation(edge_test_data)\n",
    "\n",
    "# Display results\n",
    "edge_evaluation = edge_results[\"batch_evaluation\"]\n",
    "print(f\"\\nüìä EDGE CASE EVALUATION:\")\n",
    "print(f\"Framework: {edge_evaluation.get('framework', 'N/A')}\")\n",
    "print(f\"Accuracy: {edge_evaluation.get('accuracy', 0):.3f}\")\n",
    "\n",
    "# Display individual edge case results\n",
    "print(\"\\nüìã EDGE CASE RESULTS:\")\n",
    "edge_individual = edge_results[\"individual_results\"]\n",
    "for i, result in enumerate(edge_individual, 1):\n",
    "    status = \"‚úÖ\" if result[\"correct\"] else \"‚ùå\"\n",
    "    print(f\"{i}. {status} Predicted: {result['predicted']} | Actual: {result['actual']}\")\n",
    "\n",
    "# Calculate edge case accuracy\n",
    "edge_correct = sum(1 for r in edge_individual if r[\"correct\"])\n",
    "edge_total = len(edge_individual)\n",
    "edge_accuracy = edge_correct / edge_total if edge_total > 0 else 0\n",
    "print(f\"\\nüéØ Edge Cases Accuracy: {edge_accuracy:.3f} ({edge_correct}/{edge_total})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Dataset Sample Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample from actual dataset\n",
    "print(\"üìä Testing with Dataset Sample\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Get sample test cases from dataset\n",
    "sample_size = 10\n",
    "sample_cases = test_cases_manager.get_sample_test_cases(sample_size)\n",
    "print(f\"Selected {len(sample_cases)} random samples from dataset\")\n",
    "\n",
    "# Convert to test data format\n",
    "dataset_test_data = []\n",
    "for tc in sample_cases:\n",
    "    dataset_test_data.append({\n",
    "        \"summary\": tc.input,\n",
    "        \"doc_type\": tc.expected_output\n",
    "    })\n",
    "\n",
    "# Display sample cases\n",
    "print(\"\\nüìã DATASET SAMPLE:\")\n",
    "for i, test_data in enumerate(dataset_test_data, 1):\n",
    "    print(f\"\\n{i}. Summary: {test_data['summary'][:60]}...\")\n",
    "    print(f\"   Expected: {test_data['doc_type']}\")\n",
    "\n",
    "# Run evaluation on dataset sample\n",
    "print(\"\\nüîÑ Running Dataset Sample Evaluation...\")\n",
    "dataset_results = agent.run_batch_evaluation(dataset_test_data)\n",
    "\n",
    "# Display results\n",
    "dataset_evaluation = dataset_results[\"batch_evaluation\"]\n",
    "print(f\"\\nüìä DATASET SAMPLE EVALUATION:\")\n",
    "print(f\"Framework: {dataset_evaluation.get('framework', 'N/A')}\")\n",
    "print(f\"Number of Samples: {dataset_evaluation.get('n', 0)}\")\n",
    "print(f\"Accuracy: {dataset_evaluation.get('accuracy', 0):.3f}\")\n",
    "\n",
    "if dataset_evaluation.get('framework') == 'deepeval':\n",
    "    print(f\"Contextual Precision: {dataset_evaluation.get('contextual_precision', 0):.3f}\")\n",
    "    print(f\"Contextual Recall: {dataset_evaluation.get('contextual_recall', 0):.3f}\")\n",
    "    print(f\"Contextual Relevancy: {dataset_evaluation.get('contextual_relevancy', 0):.3f}\")\n",
    "    print(f\"Faithfulness: {dataset_evaluation.get('faithfulness', 0):.3f}\")\n",
    "    print(f\"Overall Score: {dataset_evaluation.get('overall_score', 0):.3f}\")\n",
    "\n",
    "# Display individual results\n",
    "print(\"\\nüìã DATASET SAMPLE RESULTS:\")\n",
    "dataset_individual = dataset_results[\"individual_results\"]\n",
    "for i, result in enumerate(dataset_individual, 1):\n",
    "    status = \"‚úÖ\" if result[\"correct\"] else \"‚ùå\"\n",
    "    print(f\"{i}. {status} Predicted: {result['predicted']} | Actual: {result['actual']}\")\n",
    "\n",
    "# Calculate dataset accuracy\n",
    "dataset_correct = sum(1 for r in dataset_individual if r[\"correct\"])\n",
    "dataset_total = len(dataset_individual)\n",
    "dataset_accuracy = dataset_correct / dataset_total if dataset_total > 0 else 0\n",
    "print(f\"\\nüéØ Dataset Sample Accuracy: {dataset_accuracy:.3f} ({dataset_correct}/{dataset_total})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Summary and Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üìã DEEPEVAL INTEGRATION TEST SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n‚úÖ COMPLETED TESTS:\")\n",
    "print(\"  ‚úì Single prediction with live evaluation\")\n",
    "print(\"  ‚úì Custom test cases evaluation\")\n",
    "print(\"  ‚úì Edge cases testing\")\n",
    "print(\"  ‚úì Dataset sample evaluation\")\n",
    "\n",
    "print(\"\\nüìä KEY METRICS:\")\n",
    "print(f\"  ‚Ä¢ Custom Test Cases Accuracy: {accuracy:.3f}\")\n",
    "print(f\"  ‚Ä¢ Edge Cases Accuracy: {edge_accuracy:.3f}\")\n",
    "print(f\"  ‚Ä¢ Dataset Sample Accuracy: {dataset_accuracy:.3f}\")\n",
    "\n",
    "print(\"\\nüéØ EVALUATION FRAMEWORK:\")\n",
    "if 'deepeval_result' in locals() and deepeval_result.get('framework') == 'deepeval':\n",
    "    print(\"  ‚úì Deepeval framework is working correctly\")\n",
    "    print(\"  ‚úì Advanced metrics available (precision, recall, relevancy, faithfulness)\")\n",
    "else:\n",
    "    print(\"  ‚ö† Using fallback evaluation (Deepeval not available)\")\n",
    "    print(\"  üí° Install Deepeval and set Gemini API key for advanced metrics\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"  1. Install Deepeval: pip install deepeval\")\n",
    "print(\"  2. Set Gemini API key in .env file\")\n",
    "print(\"  3. Run production evaluation on full dataset\")\n",
    "print(\"  4. Fine-tune model based on evaluation results\")\n",
    "print(\"  5. Implement confidence thresholds for production\")\n",
    "\n",
    "print(\"\\nüéâ Deepeval integration testing complete!\")\n",
    "print(\"The summary classification system is ready for production use.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
