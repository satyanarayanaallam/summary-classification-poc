{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c904e1f6",
   "metadata": {},
   "source": [
    "# Triplet Manual Test Notebook\n",
    "\n",
    "This notebook helps you test the complete summary classification pipeline:\n",
    "1. **Load document summaries** from `data/summaries.json`\n",
    "2. **Extract triplets** using the project's TripletService\n",
    "3. **Index triplets** in FAISS vector store with Sentence Transformers embeddings\n",
    "4. **Run similarity queries** to test document classification\n",
    "5. **Evaluate results** with various search scenarios\n",
    "\n",
    "The notebook uses the project's modules and follows the complete pipeline from summaries → triplets → vector store → retrieval.\n",
    "\n",
    "Notes:\n",
    "- Run this notebook in the project root so relative paths resolve correctly.\n",
    "- Cells include guards so the notebook can run without optional dependencies installed.\n",
    "- This tests the Sentence Transformers integration with your actual data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "481cdd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:triplet_notebook:Features: pandas=True, sentence_transformers=True, faiss=True\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Install and import dependencies (guards)\n",
    "import importlib\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"triplet_notebook\")\n",
    "\n",
    "# Feature flags for optional libraries\n",
    "HAS_PANDAS = importlib.util.find_spec(\"pandas\") is not None\n",
    "HAS_PYARROW = importlib.util.find_spec(\"pyarrow\") is not None\n",
    "HAS_SENT_TRANS = importlib.util.find_spec(\"sentence_transformers\") is not None\n",
    "HAS_FAISS = importlib.util.find_spec(\"faiss\") is not None or importlib.util.find_spec(\"faiss-cpu\") is not None\n",
    "HAS_RAPIDFUZZ = importlib.util.find_spec(\"rapidfuzz\") is not None\n",
    "\n",
    "if not HAS_PANDAS:\n",
    "    logger.warning(\"pandas is not available. Please install it to run loaders: pip install pandas\")\n",
    "else:\n",
    "    import pandas as pd\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "import typing\n",
    "import numpy as np\n",
    "\n",
    "logger.info(f\"Features: pandas={HAS_PANDAS}, sentence_transformers={HAS_SENT_TRANS}, faiss={HAS_FAISS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b99fe2b",
   "metadata": {},
   "source": [
    "## Section 2: Configure paths and constants\n",
    "Define file paths for sample data and settings used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d59f8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths configured:\n",
      "DATA_DIR=D:\\Projects\\Gen AI Projects\\summary-classification-poc\\data\n",
      "SAMPLE_JSON=D:\\Projects\\Gen AI Projects\\summary-classification-poc\\data\\summaries.json\n",
      "SQLITE_DB=D:\\Projects\\Gen AI Projects\\summary-classification-poc\\data\\triplets.db\n"
     ]
    }
   ],
   "source": [
    "# Section 2: constants\n",
    "ROOT = Path(\"../\").resolve()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "NOTEBOOKS_DIR = ROOT / \"notebooks\"\n",
    "SAMPLE_JSON = DATA_DIR / \"summaries.json\"\n",
    "SAMPLE_JSONL = DATA_DIR / \"summaries.jsonl\"\n",
    "SAMPLE_CSV = DATA_DIR / \"summaries.csv\"\n",
    "SAMPLE_PARQUET = DATA_DIR / \"summaries.parquet\"\n",
    "SQLITE_DB = ROOT / \"data\" / \"triplets.db\"\n",
    "INDEX_DIR = ROOT / \"indexes\"\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "VERBOSE = True\n",
    "\n",
    "print(\"Paths configured:\")\n",
    "print(f\"DATA_DIR={DATA_DIR}\")\n",
    "print(f\"SAMPLE_JSON={SAMPLE_JSON}\")\n",
    "print(f\"SQLITE_DB={SQLITE_DB}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d0bdeb",
   "metadata": {},
   "source": [
    "## Section 3: Load triplets from CSV / TSV / JSONL / Parquet\n",
    "Helper functions to load common file formats and return a standardized pandas.DataFrame with columns ['subject','predicate','object','source','row_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b0a2ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 300 summaries from D:\\Projects\\Gen AI Projects\\summary-classification-poc\\data\\summaries.json\n",
      "Extracted 50 triplets from 50 summaries\n",
      "Document types found: {'INVOICE': 50}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>triplet_text</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>doc_code</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>identifier</td>\n",
       "      <td>invoice</td>\n",
       "      <td>id identifier invoice</td>\n",
       "      <td>INVOICE</td>\n",
       "      <td>INV001</td>\n",
       "      <td>This invoice (INV001) itemizes charges for pro...</td>\n",
       "      <td>D:\\Projects\\Gen AI Projects\\summary-classifica...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id</td>\n",
       "      <td>identifier</td>\n",
       "      <td>inv002</td>\n",
       "      <td>id identifier inv002</td>\n",
       "      <td>INVOICE</td>\n",
       "      <td>INV002</td>\n",
       "      <td>A detailed billing notice identified as INV002...</td>\n",
       "      <td>D:\\Projects\\Gen AI Projects\\summary-classifica...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id</td>\n",
       "      <td>identifier</td>\n",
       "      <td>invoice</td>\n",
       "      <td>id identifier invoice</td>\n",
       "      <td>INVOICE</td>\n",
       "      <td>INV003</td>\n",
       "      <td>Invoice INV003 records a transaction between t...</td>\n",
       "      <td>D:\\Projects\\Gen AI Projects\\summary-classifica...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id</td>\n",
       "      <td>identifier</td>\n",
       "      <td>inv004</td>\n",
       "      <td>id identifier inv004</td>\n",
       "      <td>INVOICE</td>\n",
       "      <td>INV004</td>\n",
       "      <td>A detailed billing notice identified as INV004...</td>\n",
       "      <td>D:\\Projects\\Gen AI Projects\\summary-classifica...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id</td>\n",
       "      <td>identifier</td>\n",
       "      <td>invoice</td>\n",
       "      <td>id identifier invoice</td>\n",
       "      <td>INVOICE</td>\n",
       "      <td>INV005</td>\n",
       "      <td>Invoice INV005 records a transaction between t...</td>\n",
       "      <td>D:\\Projects\\Gen AI Projects\\summary-classifica...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject   predicate   object           triplet_text doc_type doc_code  \\\n",
       "0      id  identifier  invoice  id identifier invoice  INVOICE   INV001   \n",
       "1      id  identifier   inv002   id identifier inv002  INVOICE   INV002   \n",
       "2      id  identifier  invoice  id identifier invoice  INVOICE   INV003   \n",
       "3      id  identifier   inv004   id identifier inv004  INVOICE   INV004   \n",
       "4      id  identifier  invoice  id identifier invoice  INVOICE   INV005   \n",
       "\n",
       "                                             summary  \\\n",
       "0  This invoice (INV001) itemizes charges for pro...   \n",
       "1  A detailed billing notice identified as INV002...   \n",
       "2  Invoice INV003 records a transaction between t...   \n",
       "3  A detailed billing notice identified as INV004...   \n",
       "4  Invoice INV005 records a transaction between t...   \n",
       "\n",
       "                                              source  row_id  \n",
       "0  D:\\Projects\\Gen AI Projects\\summary-classifica...       0  \n",
       "1  D:\\Projects\\Gen AI Projects\\summary-classifica...       1  \n",
       "2  D:\\Projects\\Gen AI Projects\\summary-classifica...       2  \n",
       "3  D:\\Projects\\Gen AI Projects\\summary-classifica...       3  \n",
       "4  D:\\Projects\\Gen AI Projects\\summary-classifica...       4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _ensure_df_columns(df: \"pd.DataFrame\") -> \"pd.DataFrame\":\n",
    "    # Normalize column names to lower and ensure required columns exist\n",
    "    df = df.copy()\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    required = [\"subject\",\"predicate\",\"object\"]\n",
    "    for col in required:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    # preserve provenance\n",
    "    if \"source\" not in df.columns:\n",
    "        df[\"source\"] = None\n",
    "    if \"row_id\" not in df.columns:\n",
    "        df[\"row_id\"] = range(len(df))\n",
    "    return df[[\"subject\",\"predicate\",\"object\",\"source\",\"row_id\"]]\n",
    "\n",
    "\n",
    "def load_csv(path: \"Path\", sep=\",\") -> \"pd.DataFrame\":\n",
    "    if not HAS_PANDAS:\n",
    "        raise RuntimeError(\"pandas not available\")\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    df = _ensure_df_columns(df)\n",
    "    df[\"source\"] = str(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_jsonl(path: \"Path\") -> \"pd.DataFrame\":\n",
    "    if not HAS_PANDAS:\n",
    "        raise RuntimeError(\"pandas not available\")\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            rows.append({\n",
    "                \"subject\": obj.get(\"subject\"),\n",
    "                \"predicate\": obj.get(\"predicate\"),\n",
    "                \"object\": obj.get(\"object\"),\n",
    "                \"source\": str(path),\n",
    "                \"row_id\": i,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    return _ensure_df_columns(df)\n",
    "\n",
    "\n",
    "def load_parquet(path: \"Path\") -> \"pd.DataFrame\":\n",
    "    if not HAS_PANDAS or not HAS_PYARROW:\n",
    "        raise RuntimeError(\"parquet support requires pandas + pyarrow\")\n",
    "    df = pd.read_parquet(path)\n",
    "    df = _ensure_df_columns(df)\n",
    "    df[\"source\"] = str(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_any(path: \"Path\") -> \"pd.DataFrame\":\n",
    "    path = Path(path)\n",
    "    if path.suffix.lower() in (\".csv\",):\n",
    "        return load_csv(path, sep=\",\")\n",
    "    if path.suffix.lower() in (\".tsv\", \".txt\"):\n",
    "        return load_csv(path, sep=\"\\t\")\n",
    "    if path.suffix.lower() in (\".jsonl\", \".ndjson\"):\n",
    "        return load_jsonl(path)\n",
    "    if path.suffix.lower() in (\".parquet\", \".pq\"):\n",
    "        return load_parquet(path)\n",
    "    if path.suffix.lower() in (\".json\",):\n",
    "        # assume full JSON array\n",
    "        if not HAS_PANDAS:\n",
    "            raise RuntimeError(\"pandas required for json array loading\")\n",
    "        df = pd.read_json(path)\n",
    "        df[\"source\"] = str(path)\n",
    "        return _ensure_df_columns(df)\n",
    "    raise ValueError(f\"Unsupported file: {path}\")\n",
    "\n",
    "\n",
    "# Load summaries data and convert to triplets\n",
    "if SAMPLE_JSON.exists():\n",
    "    try:\n",
    "        # Load summaries data\n",
    "        with open(SAMPLE_JSON, 'r', encoding='utf-8') as f:\n",
    "            summaries_data = json.load(f)\n",
    "        print(f\"Loaded {len(summaries_data)} summaries from {SAMPLE_JSON}\")\n",
    "        \n",
    "        # Import our project modules\n",
    "        import sys\n",
    "        sys.path.append(str(ROOT))\n",
    "        from src.services.triplet_service import TripletService\n",
    "        from src.utils.normalization import triplet_to_text\n",
    "        \n",
    "        # Initialize triplet service\n",
    "        triplet_service = TripletService()\n",
    "        \n",
    "        # Convert summaries to triplets\n",
    "        all_triplets = []\n",
    "        for i, item in enumerate(summaries_data[:50]):  # Process first 50 for testing\n",
    "            summary = item.get('summary', '')\n",
    "            doc_type = item.get('doc_type', '')\n",
    "            doc_code = item.get('doc_code', '')\n",
    "            \n",
    "            # Extract triplets from summary\n",
    "            triplets = triplet_service.extract_and_normalize(summary)\n",
    "            \n",
    "            for triplet in triplets:\n",
    "                triplet_text = triplet_to_text(triplet)\n",
    "                all_triplets.append({\n",
    "                    'subject': triplet[0],\n",
    "                    'predicate': triplet[1], \n",
    "                    'object': triplet[2],\n",
    "                    'triplet_text': triplet_text,\n",
    "                    'doc_type': doc_type,\n",
    "                    'doc_code': doc_code,\n",
    "                    'summary': summary,\n",
    "                    'source': str(SAMPLE_JSON),\n",
    "                    'row_id': len(all_triplets)\n",
    "                })\n",
    "        \n",
    "        # Create DataFrame from triplets\n",
    "        if HAS_PANDAS:\n",
    "            df = pd.DataFrame(all_triplets)\n",
    "            print(f\"Extracted {len(df)} triplets from {len(summaries_data[:50])} summaries\")\n",
    "            print(f\"Document types found: {df['doc_type'].value_counts().to_dict()}\")\n",
    "            display(df.head())\n",
    "        else:\n",
    "            print(f\"Extracted {len(all_triplets)} triplets (pandas not available for display)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Error loading summaries:\", e)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Sample JSON not found at\", SAMPLE_JSON)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbc7503",
   "metadata": {},
   "source": [
    "## Section 4: Validate Triplet Schema and Types\n",
    "Functions to confirm required fields are present, types are strings, and to report invalid rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab3eac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'missing': 0, 'nonstring': 0, 'rows': [], 'total_rows': 50}\n"
     ]
    }
   ],
   "source": [
    "def validate_triplets(df: \"pd.DataFrame\") -> dict:\n",
    "    issues = {\"missing\":0, \"nonstring\":0, \"rows\":[]}\n",
    "    required = [\"subject\",\"predicate\",\"object\"]\n",
    "    for i,row in df.iterrows():\n",
    "        row_issues = []\n",
    "        for c in required:\n",
    "            if pd.isna(row.get(c)) or row.get(c) is None:\n",
    "                row_issues.append(f\"missing_{c}\")\n",
    "            elif not isinstance(row.get(c), str):\n",
    "                row_issues.append(f\"nonstring_{c}\")\n",
    "        if row_issues:\n",
    "            issues[\"rows\"].append({\"row_id\": row.get(\"row_id\", i), \"issues\": row_issues})\n",
    "    issues[\"missing\"] = sum(1 for r in issues[\"rows\"] if any(\"missing\" in i for i in r[\"issues\"]))\n",
    "    issues[\"nonstring\"] = sum(1 for r in issues[\"rows\"] if any(\"nonstring\" in i for i in r[\"issues\"]))\n",
    "    issues[\"total_rows\"] = len(df)\n",
    "    return issues\n",
    "\n",
    "# Example usage\n",
    "if 'df' in globals():\n",
    "    print(validate_triplets(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08552f40",
   "metadata": {},
   "source": [
    "## Section 5.5: Test FAISS Vector Store with Sentence Transformers\n",
    "\n",
    "Test the complete pipeline: summaries → triplets → FAISS vector store → similarity search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd394669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing FAISS Vector Store with Sentence Transformers...\n",
      "Using Sentence Transformers model 'all-MiniLM-L6-v2' with dimension 384\n",
      "Adding 50 triplets to vector store...\n",
      "✓ Vector store initialized and populated successfully!\n",
      "✓ Model dimension: 384\n",
      "✓ Total triplets indexed: 50\n",
      "\n",
      "============================================================\n",
      "TESTING SIMILARITY SEARCH\n",
      "============================================================\n",
      "\n",
      "Query: 'invoice has_amount <AMOUNT>'\n",
      "Found 3 results:\n",
      "  1. Score: 0.4868 | INVOICE | id identifier invoice\n",
      "  2. Score: 0.4868 | INVOICE | id identifier invoice\n",
      "  3. Score: 0.4868 | INVOICE | id identifier invoice\n",
      "\n",
      "Query: 'payment date <DATE>'\n",
      "Found 3 results:\n",
      "  1. Score: 0.1923 | INVOICE | id identifier invoice\n",
      "  2. Score: 0.1923 | INVOICE | id identifier invoice\n",
      "  3. Score: 0.1923 | INVOICE | id identifier invoice\n",
      "\n",
      "Query: 'organization issued_by'\n",
      "Found 3 results:\n",
      "  1. Score: 0.3153 | INVOICE | id identifier invoice\n",
      "  2. Score: 0.3153 | INVOICE | id identifier invoice\n",
      "  3. Score: 0.3153 | INVOICE | id identifier invoice\n",
      "\n",
      "Query: 'bank statement account'\n",
      "Found 3 results:\n",
      "  1. Score: 0.1931 | INVOICE | id identifier invoice\n",
      "  2. Score: 0.1931 | INVOICE | id identifier invoice\n",
      "  3. Score: 0.1931 | INVOICE | id identifier invoice\n",
      "\n",
      "Query: 'leave request employee'\n",
      "Found 3 results:\n",
      "  1. Score: 0.1461 | INVOICE | id identifier invoice\n",
      "  2. Score: 0.1461 | INVOICE | id identifier invoice\n",
      "  3. Score: 0.1461 | INVOICE | id identifier invoice\n",
      "\n",
      "✓ FAISS Vector Store with Sentence Transformers is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test FAISS Vector Store with Sentence Transformers\n",
    "if 'all_triplets' in locals() and len(all_triplets) > 0:\n",
    "    try:\n",
    "        # Import our vector store\n",
    "        from src.db.vector_store import FaissVectorStore\n",
    "        \n",
    "        print(\"Initializing FAISS Vector Store with Sentence Transformers...\")\n",
    "        vector_store = FaissVectorStore(model_name=\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # Prepare data for vector store\n",
    "        triplet_texts = [t['triplet_text'] for t in all_triplets]\n",
    "        triplet_metas = [{\n",
    "            'doc_type': t['doc_type'],\n",
    "            'doc_code': t['doc_code'],\n",
    "            'subject': t['subject'],\n",
    "            'predicate': t['predicate'],\n",
    "            'object': t['object'],\n",
    "            'summary': t['summary'][:100] + \"...\" if len(t['summary']) > 100 else t['summary']\n",
    "        } for t in all_triplets]\n",
    "        \n",
    "        print(f\"Adding {len(triplet_texts)} triplets to vector store...\")\n",
    "        vector_store.add(triplet_texts, triplet_metas)\n",
    "        \n",
    "        print(\"✓ Vector store initialized and populated successfully!\")\n",
    "        print(f\"✓ Model dimension: {vector_store._dim}\")\n",
    "        print(f\"✓ Total triplets indexed: {len(triplet_texts)}\")\n",
    "        \n",
    "        # Test some queries\n",
    "        test_queries = [\n",
    "            \"invoice has_amount <AMOUNT>\",\n",
    "            \"payment date <DATE>\",\n",
    "            \"organization issued_by\",\n",
    "            \"bank statement account\",\n",
    "            \"leave request employee\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TESTING SIMILARITY SEARCH\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"\\nQuery: '{query}'\")\n",
    "            results = vector_store.query(query, top_k=3)\n",
    "            \n",
    "            if results:\n",
    "                print(f\"Found {len(results)} results:\")\n",
    "                for i, result in enumerate(results, 1):\n",
    "                    score = result.get('score', 0)\n",
    "                    doc_type = result.get('doc_type', 'N/A')\n",
    "                    subject = result.get('subject', 'N/A')\n",
    "                    predicate = result.get('predicate', 'N/A')\n",
    "                    object_val = result.get('object', 'N/A')\n",
    "                    print(f\"  {i}. Score: {score:.4f} | {doc_type} | {subject} {predicate} {object_val}\")\n",
    "            else:\n",
    "                print(\"  No results found\")\n",
    "        \n",
    "        print(\"\\n✓ FAISS Vector Store with Sentence Transformers is working correctly!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing FAISS vector store: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"No triplets available. Please run the previous cell first to extract triplets from summaries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8626e49",
   "metadata": {},
   "source": [
    "## Section 5: Preprocess Triplets (normalization, deduplication)\n",
    "Clean text fields, lowercase optionally, strip whitespace, normalize unicode, and drop duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b1422cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: rows before=50, after=20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>triplet_text</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>doc_code</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>identifier</td>\n",
       "      <td>invoice</td>\n",
       "      <td>id identifier invoice</td>\n",
       "      <td>INVOICE</td>\n",
       "      <td>INV001</td>\n",
       "      <td>This invoice (INV001) itemizes charges for pro...</td>\n",
       "      <td>D:\\Projects\\Gen AI Projects\\summary-classifica...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id</td>\n",
       "      <td>identifier</td>\n",
       "      <td>inv002</td>\n",
       "      <td>id identifier inv002</td>\n",
       "      <td>INVOICE</td>\n",
       "      <td>INV002</td>\n",
       "      <td>A detailed billing notice identified as INV002...</td>\n",
       "      <td>D:\\Projects\\Gen AI Projects\\summary-classifica...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id</td>\n",
       "      <td>identifier</td>\n",
       "      <td>inv004</td>\n",
       "      <td>id identifier inv004</td>\n",
       "      <td>INVOICE</td>\n",
       "      <td>INV004</td>\n",
       "      <td>A detailed billing notice identified as INV004...</td>\n",
       "      <td>D:\\Projects\\Gen AI Projects\\summary-classifica...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id</td>\n",
       "      <td>identifier</td>\n",
       "      <td>inv007</td>\n",
       "      <td>id identifier inv007</td>\n",
       "      <td>INVOICE</td>\n",
       "      <td>INV007</td>\n",
       "      <td>A detailed billing notice identified as INV007...</td>\n",
       "      <td>D:\\Projects\\Gen AI Projects\\summary-classifica...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id</td>\n",
       "      <td>identifier</td>\n",
       "      <td>inv011</td>\n",
       "      <td>id identifier inv011</td>\n",
       "      <td>INVOICE</td>\n",
       "      <td>INV011</td>\n",
       "      <td>A detailed billing notice identified as INV011...</td>\n",
       "      <td>D:\\Projects\\Gen AI Projects\\summary-classifica...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject   predicate   object           triplet_text doc_type doc_code  \\\n",
       "0      id  identifier  invoice  id identifier invoice  INVOICE   INV001   \n",
       "1      id  identifier   inv002   id identifier inv002  INVOICE   INV002   \n",
       "2      id  identifier   inv004   id identifier inv004  INVOICE   INV004   \n",
       "3      id  identifier   inv007   id identifier inv007  INVOICE   INV007   \n",
       "4      id  identifier   inv011   id identifier inv011  INVOICE   INV011   \n",
       "\n",
       "                                             summary  \\\n",
       "0  This invoice (INV001) itemizes charges for pro...   \n",
       "1  A detailed billing notice identified as INV002...   \n",
       "2  A detailed billing notice identified as INV004...   \n",
       "3  A detailed billing notice identified as INV007...   \n",
       "4  A detailed billing notice identified as INV011...   \n",
       "\n",
       "                                              source  row_id  \n",
       "0  D:\\Projects\\Gen AI Projects\\summary-classifica...       0  \n",
       "1  D:\\Projects\\Gen AI Projects\\summary-classifica...       1  \n",
       "2  D:\\Projects\\Gen AI Projects\\summary-classifica...       3  \n",
       "3  D:\\Projects\\Gen AI Projects\\summary-classifica...       6  \n",
       "4  D:\\Projects\\Gen AI Projects\\summary-classifica...      10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def preprocess_triplets(df: \"pd.DataFrame\", lowercase: bool=True, dedup: bool=True) -> \"pd.DataFrame\":\n",
    "    df = df.copy()\n",
    "    def clean_text(s):\n",
    "        if pd.isna(s):\n",
    "            return s\n",
    "        if not isinstance(s, str):\n",
    "            s = str(s)\n",
    "        s = s.strip()\n",
    "        s = unicodedata.normalize(\"NFKC\", s)\n",
    "        if lowercase:\n",
    "            s = s.lower()\n",
    "        return s\n",
    "\n",
    "    for c in [\"subject\",\"predicate\",\"object\"]:\n",
    "        df[c] = df[c].apply(clean_text)\n",
    "\n",
    "    before = len(df)\n",
    "    if dedup:\n",
    "        df = df.drop_duplicates(subset=[\"subject\",\"predicate\",\"object\"]).reset_index(drop=True)\n",
    "    after = len(df)\n",
    "    print(f\"Preprocessing: rows before={before}, after={after}\")\n",
    "    return df\n",
    "\n",
    "# Example: run preprocessing\n",
    "if 'df' in globals():\n",
    "    df_clean = preprocess_triplets(df)\n",
    "    display(df_clean.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313cc6a",
   "metadata": {},
   "source": [
    "## Section 6: Build Index (in-memory dict + optional FAISS/Annoy)\n",
    "Construct simple inverted indexes and optional vector indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f73d9dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index built, docs= 20\n"
     ]
    }
   ],
   "source": [
    "# Simple inverted index\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.by_subject = {}\n",
    "        self.by_predicate = {}\n",
    "        self.by_object = {}\n",
    "        self.docs = {}\n",
    "\n",
    "    def add(self, doc_id:int, subject:str, predicate:str, object_:str, source=None):\n",
    "        self.docs[doc_id] = dict(subject=subject, predicate=predicate, object=object_, source=source)\n",
    "        self.by_subject.setdefault(subject, set()).add(doc_id)\n",
    "        self.by_predicate.setdefault(predicate, set()).add(doc_id)\n",
    "        self.by_object.setdefault(object_, set()).add(doc_id)\n",
    "\n",
    "    def query_subject(self, subject:str):\n",
    "        return [self.docs[i] for i in sorted(self.by_subject.get(subject, []))]\n",
    "\n",
    "    def query_predicate(self, predicate:str):\n",
    "        return [self.docs[i] for i in sorted(self.by_predicate.get(predicate, []))]\n",
    "\n",
    "    def query_object(self, object_:str):\n",
    "        return [self.docs[i] for i in sorted(self.by_object.get(object_, []))]\n",
    "\n",
    "\n",
    "# Optional: vector index wrappers (FAISS minimal)\n",
    "VEC_DIM = 384\n",
    "\n",
    "class SimpleVectorIndex:\n",
    "    def __init__(self, dim:int=VEC_DIM):\n",
    "        self.dim = dim\n",
    "        self.embeddings = []\n",
    "        self.ids = []\n",
    "        self._model = None\n",
    "        if HAS_SENT_TRANS:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self._model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "            self.dim = self._model.get_sentence_embedding_dimension()\n",
    "\n",
    "    def add(self, texts:list, ids:list):\n",
    "        if not self._model:\n",
    "            raise RuntimeError(\"No embedding model available\")\n",
    "        emb = self._model.encode(texts, show_progress_bar=False, convert_to_numpy=True)\n",
    "        self.embeddings.append(emb)\n",
    "        self.ids.extend(ids)\n",
    "\n",
    "    def build_faiss(self):\n",
    "        if not HAS_FAISS:\n",
    "            raise RuntimeError(\"faiss not available\")\n",
    "        import faiss\n",
    "        import numpy as np\n",
    "        mat = np.vstack(self.embeddings)\n",
    "        index = faiss.IndexFlatIP(mat.shape[1])\n",
    "        faiss.normalize_L2(mat)\n",
    "        index.add(mat)\n",
    "        self._faiss = index\n",
    "\n",
    "    def search(self, query_text, k=5):\n",
    "        if not self._model:\n",
    "            raise RuntimeError(\"No embedding model available\")\n",
    "        q_emb = self._model.encode([query_text], convert_to_numpy=True)\n",
    "        import numpy as np\n",
    "        faiss.normalize_L2(q_emb)\n",
    "        D, I = self._faiss.search(q_emb, k)\n",
    "        res = []\n",
    "        for score, idx in zip(D[0].tolist(), I[0].tolist()):\n",
    "            if idx < len(self.ids):\n",
    "                res.append((self.ids[idx], float(score)))\n",
    "        return res\n",
    "\n",
    "\n",
    "# Example: build inverted index from df_clean\n",
    "if 'df_clean' in globals():\n",
    "    inv = InvertedIndex()\n",
    "    for i,row in df_clean.iterrows():\n",
    "        inv.add(int(row.row_id), row.subject, row.predicate, row.object, row.source)\n",
    "    print(\"Inverted index built, docs=\", len(inv.docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d55e27",
   "metadata": {},
   "source": [
    "## Section 7: Insert Triplets into SQLite and Index\n",
    "Store cleaned triplets in a SQLite table and bulk-add to the in-memory index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c14dc869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted rows into SQLite\n"
     ]
    }
   ],
   "source": [
    "def init_db(db_path: \"Path\"):\n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS triplets (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            subject TEXT,\n",
    "            predicate TEXT,\n",
    "            object TEXT,\n",
    "            source TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "\n",
    "def bulk_insert(conn: \"sqlite3.Connection\", df: \"pd.DataFrame\"):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"BEGIN TRANSACTION\")\n",
    "    for _,row in df.iterrows():\n",
    "        cur.execute(\"INSERT INTO triplets (id,subject,predicate,object,source) VALUES (?,?,?,?,?)\",\n",
    "                    (int(row.row_id), row.subject, row.predicate, row.object, row.source))\n",
    "    conn.commit()\n",
    "\n",
    "# Example: initialize DB and insert\n",
    "conn = init_db(SQLITE_DB)\n",
    "if 'df_clean' in globals():\n",
    "    bulk_insert(conn, df_clean)\n",
    "    print(\"Inserted rows into SQLite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c7561",
   "metadata": {},
   "source": [
    "## Section 8: Manual Querying - Exact and Pattern Match\n",
    "Examples for exact lookups, predicate filters, and pattern matching using SQL and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e594e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject lookup for 'acute myocardial infarction':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, subject, predicate, object, source]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>triplet_text</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>doc_code</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subject, predicate, object, triplet_text, doc_type, doc_code, summary, source, row_id]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exact lookup: subject\n",
    "\n",
    "def sql_query(conn, sql, params=()):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, params)\n",
    "    cols = [c[0] for c in cur.description]\n",
    "    rows = cur.fetchall()\n",
    "    import pandas as pd\n",
    "    return pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "if 'inv' in globals():\n",
    "    print(\"Subject lookup for 'acute myocardial infarction':\")\n",
    "    res = inv.query_subject('acute myocardial infarction')\n",
    "    for r in res[:5]:\n",
    "        print(r)\n",
    "\n",
    "# SQL pattern match\n",
    "if conn:\n",
    "    df_sql = sql_query(conn, \"SELECT id,subject,predicate,object,source FROM triplets WHERE subject LIKE ? LIMIT 10\", (\"%infarction%\",))\n",
    "    display(df_sql)\n",
    "\n",
    "# pandas pattern match\n",
    "if 'df_clean' in globals():\n",
    "    mm = df_clean[df_clean.subject.str.contains(\"infarction\", na=False)]\n",
    "    display(mm.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5f49e",
   "metadata": {},
   "source": [
    "## Section 9: Manual Querying - Predicate Filter and Multi-hop Lookups\n",
    "Run chained traversals across predicates to find multi-hop relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1e8fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hop(inv_index: InvertedIndex, start_subject: str, predicates: list, max_hops:int=3):\n",
    "    # predicates: list of predicate strings to follow in order (or None for any)\n",
    "    paths = []\n",
    "    frontier = [(start_subject, [start_subject])]\n",
    "    for hop in range(min(max_hops, len(predicates))):\n",
    "        next_frontier = []\n",
    "        pred = predicates[hop]\n",
    "        for node, path in frontier:\n",
    "            # get docs where subject == node\n",
    "            doc_ids = inv_index.by_subject.get(node, set())\n",
    "            for did in doc_ids:\n",
    "                doc = inv_index.docs[did]\n",
    "                if pred is None or doc['predicate'] == pred:\n",
    "                    next_node = doc['object']\n",
    "                    next_frontier.append((next_node, path + [doc['predicate'], next_node]))\n",
    "        frontier = next_frontier\n",
    "        paths.extend(frontier)\n",
    "    return paths\n",
    "\n",
    "# Example multi-hop: subject->predicate->object->predicate->object\n",
    "if 'inv' in globals():\n",
    "    ph = multi_hop(inv, 'acute myocardial infarction', ['has_symptom', 'related_to'], max_hops=2)\n",
    "    for p in ph:\n",
    "        print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770961e5",
   "metadata": {},
   "source": [
    "## Section 10: Embedding-based Semantic Search (optional)\n",
    "Compute embeddings for texts and run top-k semantic similarity if sentence-transformers is installed. Falls back to rapidfuzz-based fuzzy matching otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd2d3bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5214c44633ba4d23aaf4d2c1f1ba419f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (20, 384)\n"
     ]
    }
   ],
   "source": [
    "def build_embeddings_for_df(df: \"pd.DataFrame\", text_field: str='object') -> np.ndarray:\n",
    "    if not HAS_SENT_TRANS:\n",
    "        raise RuntimeError(\"sentence_transformers not installed\")\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "    texts = df[text_field].astype(str).tolist()\n",
    "    emb = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "    return emb\n",
    "\n",
    "# fallback fuzzy search\n",
    "if not HAS_SENT_TRANS and HAS_RAPIDFUZZ:\n",
    "    from rapidfuzz import fuzz, process\n",
    "\n",
    "\n",
    "# Example: build embeddings if possible\n",
    "if 'df_clean' in globals() and HAS_SENT_TRANS:\n",
    "    emb = build_embeddings_for_df(df_clean, 'object')\n",
    "    print('Embeddings shape:', emb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae40cad",
   "metadata": {},
   "source": [
    "## Section 11: Run End-to-End Test Scenarios\n",
    "Execute a set of example cells that run all scenarios and summarize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f5555e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact lookup for subject=acute myocardial infarction, found=0\n",
      "Pattern search for pattern=infarction, rows=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0befcdf103c34b70b55caee189f9413c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic index build failed: name 'faiss' is not defined\n",
      "Scenarios completed, keys= ['exact', 'pattern']\n"
     ]
    }
   ],
   "source": [
    "def scenario_exact_lookup(inv_index: InvertedIndex, subject: str):\n",
    "    res = inv_index.query_subject(subject)\n",
    "    print(f\"Exact lookup for subject={subject}, found={len(res)}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def scenario_pattern_search(conn, pattern: str):\n",
    "    df = sql_query(conn, \"SELECT id,subject,predicate,object,source FROM triplets WHERE object LIKE ? LIMIT 20\", (f\"%{pattern}%\",))\n",
    "    print(f\"Pattern search for pattern={pattern}, rows={len(df)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_all_scenarios():\n",
    "    results = {}\n",
    "    if 'inv' in globals():\n",
    "        results['exact'] = scenario_exact_lookup(inv, 'acute myocardial infarction')\n",
    "    if conn:\n",
    "        results['pattern'] = scenario_pattern_search(conn, 'infarction')\n",
    "    if 'df_clean' in globals() and HAS_SENT_TRANS:\n",
    "        # semantic example\n",
    "        vid = SimpleVectorIndex()\n",
    "        texts = df_clean['object'].tolist()\n",
    "        ids = df_clean['row_id'].tolist()\n",
    "        vid.add(texts, ids)\n",
    "        try:\n",
    "            vid.build_faiss()\n",
    "            results['semantic'] = vid.search('heart attack symptoms', k=5)\n",
    "        except Exception as e:\n",
    "            print('Semantic index build failed:', e)\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    out = run_all_scenarios()\n",
    "    print('Scenarios completed, keys=', list(out.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3252cf-e023-474a-b709-7ec5c6de8018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
